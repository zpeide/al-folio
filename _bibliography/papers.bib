---
---

@inproceedings{zhu2021improving,
  title        = {Improving Long Content Question Generation with Multi-level Passage Encoding},
  author       = {Zhu, Peide},
  booktitle    = {Pacific Rim International Conference on Artificial Intelligence},
  abbr         = {PRICAI},
  pages        = {140--152},
  year         = {2021},
  selected=true,
  organization = {Springer}
}

@article{zhu2021evaluating,
  title={Evaluating BERT-based Rewards for Question Generation with Reinforcement Learning},
  author={Zhu, Peide and Hauff, Claudia},
  booktitle={Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval},
  abbr={ICTIR},
  pages={261--270},
  selected=true,
  year={2021}
}

@inproceedings{2022.EDM-posters.86,
 abstract = {In this work, we address the information overload issue that learners in Massive Open Online Courses (MOOCs) face when attempting to close their knowledge gaps via the use of MOOC discussion forums.  To this end, we investigate the recommendation of one-minute-resolution video clips given the textual similarity between the clips' transcripts and MOOC discussion forum entries. We first create a large-scale dataset from Khan Academy video transcripts and their forum discussions. We then investigate the effectiveness of applying pre-trained transformers-based neural retrieval models to rank video clips in response to a forum discussion. The retrieval models are trained with supervised learning and distant supervision to effectively leverage the unlabeled data---which accounts for more than 80\% of all available data. Our experimental results demonstrate that the proposed method is effective for this task, by outperforming a standard baseline by 0.208 on the absolute change in terms of precision.},
 address = {Durham, United Kingdom},
 author = {Peide Zhu and Claudia Hauff and Jie Yang},
 booktitle = {Proceedings of the 15th International Conference on Educational Data Mining},
 doi = {10.5281/zenodo.6853055},
 editor = {Antonija Mitrovic and Nigel Bosch},
 isbn = {978-1-7336736-3-1},
 month = {July},
 pages = {705--709},
 abbr  = {EDM},
 selected=true,
 publisher = {International Educational Data Mining Society},
 title = {{MOOC-Rec}: Instructional Video Clip Recommendation for {MOOC} Forum Questions},
 year = {2022}
}

@inproceedings{zhu2022unsupervised,
  title     = {Unsupervised Domain Adaptation for Question Generation with DomainData Selection and Self-training},
  author    = {Zhu, Peide and Hauff, Claudia},
  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2022},
  pages     = {2388--2401},
  abbr={NAACL},
  selected=True,
  year      = {2022}
}

@inproceedings{zhu-hauff-2022-unsupervised,
    title = "Unsupervised Domain Adaptation for Question Generation with {D}omain{D}ata Selection and Self-training",
    author = "Zhu, Peide  and
      Hauff, Claudia",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.183",
    doi = "10.18653/v1/2022.findings-naacl.183",
    pages = "2388--2401",
    abbr={EMNLP},
    selected=true,
    abstract = "Question generation (QG) approaches based on large neural models require (i) large-scale and (ii) high-quality training data. These two requirements pose difficulties for specific application domains where training data is expensive and difficult to obtain. The trained QG models{'} effectiveness can degrade significantly when they are applied on a different domain due to domain shift. In this paper, we explore an \textit{unsupervised domain adaptation} approach to combat the lack of training data and domain shift issue with domain data selection and self-training. We first present a novel answer-aware strategy for domain data selection to select data with the most similarity to a new domain. The selected data are then used as pseudo-in-domain data to retrain the QG model. We then present generation confidence guided self-training with two generation confidence modeling methods (i) generated questions{'} perplexity and (ii) the fluency score. We test our approaches on three large public datasets with different domain similarities, using a transformer-based pre-trained QG model. The results show that our proposed approaches outperform the baselines, and show the viability of unsupervised domain adaptation with answer-aware data selection and self-training on the QG task.",
}

@inproceedings{hou2019signspeaker,
  title     = {Signspeaker: A real-time, high-precision smartwatch-based sign language translator},
  author    = {Hou, Jiahui and Li, Xiang-Yang and Zhu, Peide and Wang, Zefan and Wang, Yu and Qian, Jianwei and Yang, Panlong},
  booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
  pages     = {1--15},
  year      = {2019}
}
